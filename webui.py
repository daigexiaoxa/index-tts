import json
import os
import sys
import threading
import time

import warnings

import numpy as np

warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)

import pandas as pd

current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)
sys.path.append(os.path.join(current_dir, "indextts"))

import argparse
parser = argparse.ArgumentParser(
    description="IndexTTS WebUI",
    formatter_class=argparse.ArgumentDefaultsHelpFormatter,
)
parser.add_argument("--verbose", action="store_true", default=False, help="Enable verbose mode")
parser.add_argument("--port", type=int, default=7860, help="Port to run the web UI on")
parser.add_argument("--host", type=str, default="0.0.0.0", help="Host to run the web UI on")
parser.add_argument("--model_dir", type=str, default="./checkpoints", help="Model checkpoints directory")
parser.add_argument("--fp16", action="store_true", default=False, help="Use FP16 for inference if available")
parser.add_argument("--deepspeed", action="store_true", default=False, help="Use DeepSpeed to accelerate if available")
parser.add_argument("--cuda_kernel", action="store_true", default=False, help="Use CUDA kernel for inference if available")
parser.add_argument("--gui_seg_tokens", type=int, default=120, help="GUI: Max tokens per generation segment")
cmd_args = parser.parse_args()

if not os.path.exists(cmd_args.model_dir):
    print(f"Model directory {cmd_args.model_dir} does not exist. Please download the model first.")
    sys.exit(1)

for file in [
    "bpe.model",
    "gpt.pth",
    "config.yaml",
    "s2mel.pth",
    "wav2vec2bert_stats.pt"
]:
    file_path = os.path.join(cmd_args.model_dir, file)
    if not os.path.exists(file_path):
        print(f"Required file {file_path} does not exist. Please download it.")
        sys.exit(1)

import gradio as gr
from indextts.infer_v2 import IndexTTS2
from tools.i18n.i18n import I18nAuto

i18n = I18nAuto(language="Auto")
MODE = 'local'
tts = IndexTTS2(model_dir=cmd_args.model_dir,
                cfg_path=os.path.join(cmd_args.model_dir, "config.yaml"),
                use_fp16=cmd_args.fp16,
                use_deepspeed=cmd_args.deepspeed,
                use_cuda_kernel=cmd_args.cuda_kernel,
                )
# æ”¯æŒçš„è¯­è¨€åˆ—è¡¨
LANGUAGES = {
    "ä¸­æ–‡": "zh_CN",
    "English": "en_US"
}
EMO_CHOICES = [i18n("ä¸éŸ³è‰²å‚è€ƒéŸ³é¢‘ç›¸åŒ"),
                i18n("ä½¿ç”¨æƒ…æ„Ÿå‚è€ƒéŸ³é¢‘"),
                i18n("ä½¿ç”¨æƒ…æ„Ÿå‘é‡æ§åˆ¶"),
                i18n("ä½¿ç”¨æƒ…æ„Ÿæè¿°æ–‡æœ¬æ§åˆ¶")]
EMO_CHOICES_BASE = EMO_CHOICES[:3]  # åŸºç¡€é€‰é¡¹
EMO_CHOICES_EXPERIMENTAL = EMO_CHOICES  # å…¨éƒ¨é€‰é¡¹ï¼ˆåŒ…æ‹¬æ–‡æœ¬æè¿°ï¼‰

os.makedirs("outputs/tasks",exist_ok=True)
os.makedirs("prompts",exist_ok=True)

MAX_LENGTH_TO_USE_SPEED = 70
with open("examples/cases.jsonl", "r", encoding="utf-8") as f:
    example_cases = []
    for line in f:
        line = line.strip()
        if not line:
            continue
        example = json.loads(line)
        if example.get("emo_audio",None):
            emo_audio_path = os.path.join("examples",example["emo_audio"])
        else:
            emo_audio_path = None
        example_cases.append([os.path.join("examples", example.get("prompt_audio", "sample_prompt.wav")),
                              EMO_CHOICES[example.get("emo_mode",0)],
                              example.get("text"),
                             emo_audio_path,
                             example.get("emo_weight",1.0),
                             example.get("emo_text",""),
                             example.get("emo_vec_1",0),
                             example.get("emo_vec_2",0),
                             example.get("emo_vec_3",0),
                             example.get("emo_vec_4",0),
                             example.get("emo_vec_5",0),
                             example.get("emo_vec_6",0),
                             example.get("emo_vec_7",0),
                             example.get("emo_vec_8",0),
                             example.get("emo_text") is not None]
                             )

def normalize_emo_vec(emo_vec):
    # emotion factors for better user experience
    k_vec = [0.75,0.70,0.80,0.80,0.75,0.75,0.55,0.45]
    tmp = np.array(k_vec) * np.array(emo_vec)
    if np.sum(tmp) > 0.8:
        tmp = tmp * 0.8/ np.sum(tmp)
    return tmp.tolist()

def gen_single(emo_control_method,prompt, text,
               emo_ref_path, emo_weight,
               vec1, vec2, vec3, vec4, vec5, vec6, vec7, vec8,
               emo_text,emo_random,
               max_text_tokens_per_segment=120,
                *args, progress=gr.Progress()):
    output_path = None
    if not output_path:
        output_path = os.path.join("outputs", f"spk_{int(time.time())}.wav")
    # set gradio progress
    tts.gr_progress = progress
    do_sample, top_p, top_k, temperature, \
        length_penalty, num_beams, repetition_penalty, max_mel_tokens = args
    kwargs = {
        "do_sample": bool(do_sample),
        "top_p": float(top_p),
        "top_k": int(top_k) if int(top_k) > 0 else None,
        "temperature": float(temperature),
        "length_penalty": float(length_penalty),
        "num_beams": num_beams,
        "repetition_penalty": float(repetition_penalty),
        "max_mel_tokens": int(max_mel_tokens),
        # "typical_sampling": bool(typical_sampling),
        # "typical_mass": float(typical_mass),
    }
    if type(emo_control_method) is not int:
        emo_control_method = emo_control_method.value
    if emo_control_method == 0:  # emotion from speaker
        emo_ref_path = None  # remove external reference audio
    if emo_control_method == 1:  # emotion from reference audio
        # normalize emo_alpha for better user experience
        emo_weight = emo_weight * 0.8
        pass
    if emo_control_method == 2:  # emotion from custom vectors
        vec = [vec1, vec2, vec3, vec4, vec5, vec6, vec7, vec8]
        vec = normalize_emo_vec(vec)
    else:
        # don't use the emotion vector inputs for the other modes
        vec = None

    if emo_text == "":
        # erase empty emotion descriptions; `infer()` will then automatically use the main prompt
        emo_text = None

    print(f"Emo control mode:{emo_control_method},weight:{emo_weight},vec:{vec}")
    output = tts.infer(spk_audio_prompt=prompt, text=text,
                       output_path=output_path,
                       emo_audio_prompt=emo_ref_path, emo_alpha=emo_weight,
                       emo_vector=vec,
                       use_emo_text=(emo_control_method==3), emo_text=emo_text,use_random=emo_random,
                       verbose=cmd_args.verbose,
                       max_text_tokens_per_segment=int(max_text_tokens_per_segment),
                       **kwargs)
    return gr.update(value=output,visible=True)

def gen_batch(batch_data, output_dir_option, custom_output_dir, progress=gr.Progress()):
    """
    Process batch inference for multiple triples of (reference, emotion, text).
    
    Args:
        batch_data (str): CSV/JSON string or file containing batch data
        output_dir_option (str): "auto" or "custom"
        custom_output_dir (str): Custom output directory path
        progress: Gradio progress tracker
    """
    import csv
    import io
    
    if not batch_data or not batch_data.strip():
        return "âŒ No batch data provided", None
    
    # Set gradio progress
    tts.gr_progress = progress
    
    try:
        # Parse batch data (expecting CSV format with columns: spk_audio_prompt, text, emo_audio_prompt, emo_weight, etc.)
        batch_inputs = []
        
        # Try to parse as CSV
        try:
            csv_reader = csv.DictReader(io.StringIO(batch_data.strip()))
            for row in csv_reader:
                if 'spk_audio_prompt' in row and 'text' in row:
                    batch_item = {
                        'spk_audio_prompt': row.get('spk_audio_prompt', '').strip(),
                        'text': row.get('text', '').strip(),
                        'emo_audio_prompt': row.get('emo_audio_prompt', '').strip() or None,
                        'emo_alpha': float(row.get('emo_alpha', '1.0')),
                        'emo_text': row.get('emo_text', '').strip() or None,
                        'max_text_tokens_per_segment': int(row.get('max_text_tokens_per_segment', '120')),
                    }
                    if batch_item['spk_audio_prompt'] and batch_item['text']:
                        batch_inputs.append(batch_item)
        except Exception as e:
            return f"âŒ Error parsing CSV data: {str(e)}", None
        
        if not batch_inputs:
            return "âŒ No valid batch inputs found. Please ensure CSV has 'spk_audio_prompt' and 'text' columns.", None
        
        # Determine output directory
        if output_dir_option == "custom" and custom_output_dir.strip():
            output_dir = custom_output_dir.strip()
        else:
            output_dir = os.path.join("outputs", f"batch_{int(time.time())}")
        
        # Process batch
        outputs = tts.infer_batch(
            batch_inputs=batch_inputs,
            output_dir=output_dir,
            verbose=cmd_args.verbose,
            # Default generation parameters
            do_sample=True,
            top_p=0.8,
            top_k=30,
            temperature=0.8,
            length_penalty=0.0,
            num_beams=3,
            repetition_penalty=10.0,
            max_mel_tokens=1500
        )
        
        # Create result summary
        successful_outputs = [o for o in outputs if o is not None]
        failed_count = len(outputs) - len(successful_outputs)
        
        result_message = f"âœ… Batch processing completed!\n"
        result_message += f"ğŸ“Š Total items: {len(batch_inputs)}\n"
        result_message += f"âœ… Successful: {len(successful_outputs)}\n"
        result_message += f"âŒ Failed: {failed_count}\n"
        result_message += f"ğŸ“ Output directory: {output_dir}\n"
        
        if successful_outputs:
            result_message += f"\nğŸ“‹ Generated files:\n"
            for i, output_path in enumerate(successful_outputs[:10]):  # Show first 10
                if output_path:
                    result_message += f"  {i+1}. {os.path.basename(output_path)}\n"
            if len(successful_outputs) > 10:
                result_message += f"  ... and {len(successful_outputs) - 10} more files\n"
        
        # Return the first successful output as sample
        sample_output = None
        for output in successful_outputs:
            if output and os.path.exists(output):
                sample_output = output
                break
                
        return result_message, sample_output
        
    except Exception as e:
        import traceback
        error_msg = f"âŒ Error during batch processing: {str(e)}\n"
        if cmd_args.verbose:
            error_msg += f"\nTraceback:\n{traceback.format_exc()}"
        return error_msg, None

def update_prompt_audio():
    update_button = gr.update(interactive=True)
    return update_button

with gr.Blocks(title="IndexTTS Demo") as demo:
    mutex = threading.Lock()
    gr.HTML('''
    <h2><center>IndexTTS2: A Breakthrough in Emotionally Expressive and Duration-Controlled Auto-Regressive Zero-Shot Text-to-Speech</h2>
<p align="center">
<a href='https://arxiv.org/abs/2506.21619'><img src='https://img.shields.io/badge/ArXiv-2506.21619-red'></a>
</p>
    ''')

    with gr.Tab(i18n("éŸ³é¢‘ç”Ÿæˆ")):
        with gr.Row():
            os.makedirs("prompts",exist_ok=True)
            prompt_audio = gr.Audio(label=i18n("éŸ³è‰²å‚è€ƒéŸ³é¢‘"),key="prompt_audio",
                                    sources=["upload","microphone"],type="filepath")
            prompt_list = os.listdir("prompts")
            default = ''
            if prompt_list:
                default = prompt_list[0]
            with gr.Column():
                input_text_single = gr.TextArea(label=i18n("æ–‡æœ¬"),key="input_text_single", placeholder=i18n("è¯·è¾“å…¥ç›®æ ‡æ–‡æœ¬"), info=f"{i18n('å½“å‰æ¨¡å‹ç‰ˆæœ¬')}{tts.model_version or '1.0'}")
                gen_button = gr.Button(i18n("ç”Ÿæˆè¯­éŸ³"), key="gen_button",interactive=True)
            output_audio = gr.Audio(label=i18n("ç”Ÿæˆç»“æœ"), visible=True,key="output_audio")
        experimental_checkbox = gr.Checkbox(label=i18n("æ˜¾ç¤ºå®éªŒåŠŸèƒ½"),value=False)
        with gr.Accordion(i18n("åŠŸèƒ½è®¾ç½®")):
            # æƒ…æ„Ÿæ§åˆ¶é€‰é¡¹éƒ¨åˆ†
            with gr.Row():
                emo_control_method = gr.Radio(
                    choices=EMO_CHOICES_BASE,
                    type="index",
                    value=EMO_CHOICES_BASE[0],label=i18n("æƒ…æ„Ÿæ§åˆ¶æ–¹å¼"))
        # æƒ…æ„Ÿå‚è€ƒéŸ³é¢‘éƒ¨åˆ†
        with gr.Group(visible=False) as emotion_reference_group:
            with gr.Row():
                emo_upload = gr.Audio(label=i18n("ä¸Šä¼ æƒ…æ„Ÿå‚è€ƒéŸ³é¢‘"), type="filepath")

        # æƒ…æ„Ÿéšæœºé‡‡æ ·
        with gr.Row(visible=False) as emotion_randomize_group:
            emo_random = gr.Checkbox(label=i18n("æƒ…æ„Ÿéšæœºé‡‡æ ·"), value=False)

        # æƒ…æ„Ÿå‘é‡æ§åˆ¶éƒ¨åˆ†
        with gr.Group(visible=False) as emotion_vector_group:
            with gr.Row():
                with gr.Column():
                    vec1 = gr.Slider(label=i18n("å–œ"), minimum=0.0, maximum=1.0, value=0.0, step=0.05)
                    vec2 = gr.Slider(label=i18n("æ€’"), minimum=0.0, maximum=1.0, value=0.0, step=0.05)
                    vec3 = gr.Slider(label=i18n("å“€"), minimum=0.0, maximum=1.0, value=0.0, step=0.05)
                    vec4 = gr.Slider(label=i18n("æƒ§"), minimum=0.0, maximum=1.0, value=0.0, step=0.05)
                with gr.Column():
                    vec5 = gr.Slider(label=i18n("åŒæ¶"), minimum=0.0, maximum=1.0, value=0.0, step=0.05)
                    vec6 = gr.Slider(label=i18n("ä½è½"), minimum=0.0, maximum=1.0, value=0.0, step=0.05)
                    vec7 = gr.Slider(label=i18n("æƒŠå–œ"), minimum=0.0, maximum=1.0, value=0.0, step=0.05)
                    vec8 = gr.Slider(label=i18n("å¹³é™"), minimum=0.0, maximum=1.0, value=0.0, step=0.05)

        with gr.Group(visible=False) as emo_text_group:
            with gr.Row():
                emo_text = gr.Textbox(label=i18n("æƒ…æ„Ÿæè¿°æ–‡æœ¬"),
                                      placeholder=i18n("è¯·è¾“å…¥æƒ…ç»ªæè¿°ï¼ˆæˆ–ç•™ç©ºä»¥è‡ªåŠ¨ä½¿ç”¨ç›®æ ‡æ–‡æœ¬ä½œä¸ºæƒ…ç»ªæè¿°ï¼‰"),
                                      value="",
                                      info=i18n("ä¾‹å¦‚ï¼šå§”å±ˆå·´å·´ã€å±é™©åœ¨æ‚„æ‚„é€¼è¿‘"))


        with gr.Row(visible=False) as emo_weight_group:
            emo_weight = gr.Slider(label=i18n("æƒ…æ„Ÿæƒé‡"), minimum=0.0, maximum=1.0, value=0.8, step=0.01)

        with gr.Accordion(i18n("é«˜çº§ç”Ÿæˆå‚æ•°è®¾ç½®"), open=False,visible=False) as advanced_settings_group:
            with gr.Row():
                with gr.Column(scale=1):
                    gr.Markdown(f"**{i18n('GPT2 é‡‡æ ·è®¾ç½®')}** _{i18n('å‚æ•°ä¼šå½±å“éŸ³é¢‘å¤šæ ·æ€§å’Œç”Ÿæˆé€Ÿåº¦è¯¦è§')} [Generation strategies](https://huggingface.co/docs/transformers/main/en/generation_strategies)._")
                    with gr.Row():
                        do_sample = gr.Checkbox(label="do_sample", value=True, info=i18n("æ˜¯å¦è¿›è¡Œé‡‡æ ·"))
                        temperature = gr.Slider(label="temperature", minimum=0.1, maximum=2.0, value=0.8, step=0.1)
                    with gr.Row():
                        top_p = gr.Slider(label="top_p", minimum=0.0, maximum=1.0, value=0.8, step=0.01)
                        top_k = gr.Slider(label="top_k", minimum=0, maximum=100, value=30, step=1)
                        num_beams = gr.Slider(label="num_beams", value=3, minimum=1, maximum=10, step=1)
                    with gr.Row():
                        repetition_penalty = gr.Number(label="repetition_penalty", precision=None, value=10.0, minimum=0.1, maximum=20.0, step=0.1)
                        length_penalty = gr.Number(label="length_penalty", precision=None, value=0.0, minimum=-2.0, maximum=2.0, step=0.1)
                    max_mel_tokens = gr.Slider(label="max_mel_tokens", value=1500, minimum=50, maximum=tts.cfg.gpt.max_mel_tokens, step=10, info=i18n("ç”ŸæˆTokenæœ€å¤§æ•°é‡ï¼Œè¿‡å°å¯¼è‡´éŸ³é¢‘è¢«æˆªæ–­"), key="max_mel_tokens")
                    # with gr.Row():
                    #     typical_sampling = gr.Checkbox(label="typical_sampling", value=False, info="ä¸å»ºè®®ä½¿ç”¨")
                    #     typical_mass = gr.Slider(label="typical_mass", value=0.9, minimum=0.0, maximum=1.0, step=0.1)
                with gr.Column(scale=2):
                    gr.Markdown(f'**{i18n("åˆ†å¥è®¾ç½®")}** _{i18n("å‚æ•°ä¼šå½±å“éŸ³é¢‘è´¨é‡å’Œç”Ÿæˆé€Ÿåº¦")}_')
                    with gr.Row():
                        initial_value = max(20, min(tts.cfg.gpt.max_text_tokens, cmd_args.gui_seg_tokens))
                        max_text_tokens_per_segment = gr.Slider(
                            label=i18n("åˆ†å¥æœ€å¤§Tokenæ•°"), value=initial_value, minimum=20, maximum=tts.cfg.gpt.max_text_tokens, step=2, key="max_text_tokens_per_segment",
                            info=i18n("å»ºè®®80~200ä¹‹é—´ï¼Œå€¼è¶Šå¤§ï¼Œåˆ†å¥è¶Šé•¿ï¼›å€¼è¶Šå°ï¼Œåˆ†å¥è¶Šç¢ï¼›è¿‡å°è¿‡å¤§éƒ½å¯èƒ½å¯¼è‡´éŸ³é¢‘è´¨é‡ä¸é«˜"),
                        )
                    with gr.Accordion(i18n("é¢„è§ˆåˆ†å¥ç»“æœ"), open=True) as segments_settings:
                        segments_preview = gr.Dataframe(
                            headers=[i18n("åºå·"), i18n("åˆ†å¥å†…å®¹"), i18n("Tokenæ•°")],
                            key="segments_preview",
                            wrap=True,
                        )
            advanced_params = [
                do_sample, top_p, top_k, temperature,
                length_penalty, num_beams, repetition_penalty, max_mel_tokens,
                # typical_sampling, typical_mass,
            ]
        
        if len(example_cases) > 2:
            example_table = gr.Examples(
                examples=example_cases[:-2],
                examples_per_page=20,
                inputs=[prompt_audio,
                        emo_control_method,
                        input_text_single,
                        emo_upload,
                        emo_weight,
                        emo_text,
                        vec1,vec2,vec3,vec4,vec5,vec6,vec7,vec8,experimental_checkbox]
            )
        elif len(example_cases) > 0:
            example_table = gr.Examples(
                examples=example_cases,
                examples_per_page=20,
                inputs=[prompt_audio,
                        emo_control_method,
                        input_text_single,
                        emo_upload,
                        emo_weight,
                        emo_text,
                        vec1, vec2, vec3, vec4, vec5, vec6, vec7, vec8, experimental_checkbox]
            )

    def on_input_text_change(text, max_text_tokens_per_segment):
        if text and len(text) > 0:
            text_tokens_list = tts.tokenizer.tokenize(text)

            segments = tts.tokenizer.split_segments(text_tokens_list, max_text_tokens_per_segment=int(max_text_tokens_per_segment))
            data = []
            for i, s in enumerate(segments):
                segment_str = ''.join(s)
                tokens_count = len(s)
                data.append([i, segment_str, tokens_count])
            return {
                segments_preview: gr.update(value=data, visible=True, type="array"),
            }
        else:
            df = pd.DataFrame([], columns=[i18n("åºå·"), i18n("åˆ†å¥å†…å®¹"), i18n("Tokenæ•°")])
            return {
                segments_preview: gr.update(value=df),
            }

    def on_method_select(emo_control_method):
        if emo_control_method == 1:  # emotion reference audio
            return (gr.update(visible=True),
                    gr.update(visible=False),
                    gr.update(visible=False),
                    gr.update(visible=False),
                    gr.update(visible=True)
                    )
        elif emo_control_method == 2:  # emotion vectors
            return (gr.update(visible=False),
                    gr.update(visible=True),
                    gr.update(visible=True),
                    gr.update(visible=False),
                    gr.update(visible=False)
                    )
        elif emo_control_method == 3:  # emotion text description
            return (gr.update(visible=False),
                    gr.update(visible=True),
                    gr.update(visible=False),
                    gr.update(visible=True),
                    gr.update(visible=True)
                    )
        else:  # 0: same as speaker voice
            return (gr.update(visible=False),
                    gr.update(visible=False),
                    gr.update(visible=False),
                    gr.update(visible=False),
                    gr.update(visible=False)
                    )

    def on_experimental_change(is_exp):
        # åˆ‡æ¢æƒ…æ„Ÿæ§åˆ¶é€‰é¡¹
        # ç¬¬ä¸‰ä¸ªè¿”å›å€¼å®é™…æ²¡æœ‰èµ·ä½œç”¨
        if is_exp:
            return gr.update(choices=EMO_CHOICES_EXPERIMENTAL, value=EMO_CHOICES_EXPERIMENTAL[0]), gr.update(visible=True),gr.update(value=example_cases)
        else:
            return gr.update(choices=EMO_CHOICES_BASE, value=EMO_CHOICES_BASE[0]), gr.update(visible=False),gr.update(value=example_cases[:-2])

    emo_control_method.select(on_method_select,
        inputs=[emo_control_method],
        outputs=[emotion_reference_group,
                 emotion_randomize_group,
                 emotion_vector_group,
                 emo_text_group,
                 emo_weight_group]
    )

    input_text_single.change(
        on_input_text_change,
        inputs=[input_text_single, max_text_tokens_per_segment],
        outputs=[segments_preview]
    )

    experimental_checkbox.change(
        on_experimental_change,
        inputs=[experimental_checkbox],
        outputs=[emo_control_method, advanced_settings_group,example_table.dataset]  # é«˜çº§å‚æ•°Accordion
    )

    max_text_tokens_per_segment.change(
        on_input_text_change,
        inputs=[input_text_single, max_text_tokens_per_segment],
        outputs=[segments_preview]
    )

    prompt_audio.upload(update_prompt_audio,
                         inputs=[],
                         outputs=[gen_button])

    gen_button.click(gen_single,
                     inputs=[emo_control_method,prompt_audio, input_text_single, emo_upload, emo_weight,
                            vec1, vec2, vec3, vec4, vec5, vec6, vec7, vec8,
                             emo_text,emo_random,
                             max_text_tokens_per_segment,
                             *advanced_params,
                     ],
                     outputs=[output_audio])

    # Batch Processing Tab
    with gr.Tab(i18n("æ‰¹é‡ç”Ÿæˆ")):
        gr.Markdown(f"### {i18n('æ‰¹é‡éŸ³é¢‘ç”Ÿæˆ')}")
        gr.Markdown(i18n("ä¸Šä¼ åŒ…å«å¤šä¸ªéŸ³é¢‘ç”Ÿæˆä»»åŠ¡çš„CSVæ–‡ä»¶ï¼Œæ”¯æŒæ‰¹é‡å¤„ç†å¤šç»„ (å‚è€ƒéŸ³é¢‘, æƒ…æ„ŸéŸ³é¢‘, æ–‡æœ¬) çš„ç»„åˆã€‚"))
        
        with gr.Row():
            with gr.Column(scale=2):
                batch_input = gr.TextArea(
                    label=i18n("æ‰¹é‡æ•°æ® (CSVæ ¼å¼)"),
                    placeholder="""spk_audio_prompt,text,emo_audio_prompt,emo_alpha,emo_text,max_text_tokens_per_segment
examples/sample_prompt.wav,"Hello world, this is a test.",examples/emo_sample.wav,1.0,"happy",120
examples/sample_prompt2.wav,"Another test sentence.",examples/emo_sample2.wav,0.8,"excited",100""",
                    lines=10,
                    info=i18n("è¯·è¾“å…¥CSVæ ¼å¼çš„æ‰¹é‡æ•°æ®ã€‚å¿…éœ€åˆ—: spk_audio_prompt, textã€‚å¯é€‰åˆ—: emo_audio_prompt, emo_alpha, emo_text, max_text_tokens_per_segment")
                )
                
                with gr.Row():
                    output_dir_option = gr.Radio(
                        choices=["auto", "custom"],
                        value="auto",
                        label=i18n("è¾“å‡ºç›®å½•é€‰æ‹©"),
                        info=i18n("auto: è‡ªåŠ¨ç”Ÿæˆç›®å½•ï¼Œcustom: è‡ªå®šä¹‰ç›®å½•")
                    )
                    custom_output_dir = gr.Textbox(
                        label=i18n("è‡ªå®šä¹‰è¾“å‡ºç›®å½•"),
                        placeholder="outputs/my_batch",
                        visible=False
                    )
                
                batch_gen_button = gr.Button(i18n("å¼€å§‹æ‰¹é‡ç”Ÿæˆ"), variant="primary")
                
            with gr.Column(scale=1):
                batch_result = gr.TextArea(
                    label=i18n("æ‰¹é‡å¤„ç†ç»“æœ"),
                    lines=10,
                    interactive=False
                )
                batch_sample_output = gr.Audio(
                    label=i18n("ç¤ºä¾‹è¾“å‡º"),
                    visible=True
                )
        
        # Add example CSV data
        with gr.Accordion(i18n("CSVæ ¼å¼è¯´æ˜"), open=False):
            gr.Markdown("""
            **å¿…éœ€åˆ—:**
            - `spk_audio_prompt`: éŸ³è‰²å‚è€ƒéŸ³é¢‘æ–‡ä»¶è·¯å¾„
            - `text`: è¦åˆæˆçš„æ–‡æœ¬å†…å®¹
            
            **å¯é€‰åˆ—:**
            - `emo_audio_prompt`: æƒ…æ„Ÿå‚è€ƒéŸ³é¢‘æ–‡ä»¶è·¯å¾„
            - `emo_alpha`: æƒ…æ„Ÿæƒé‡ (0.0-1.0, é»˜è®¤1.0)
            - `emo_text`: æƒ…æ„Ÿæè¿°æ–‡æœ¬
            - `max_text_tokens_per_segment`: åˆ†å¥æœ€å¤§Tokenæ•° (é»˜è®¤120)
            
            **ç¤ºä¾‹CSV:**
            ```csv
            spk_audio_prompt,text,emo_audio_prompt,emo_alpha,emo_text,max_text_tokens_per_segment
            examples/speaker1.wav,"Hello, how are you?",examples/happy.wav,1.0,"cheerful",120
            examples/speaker2.wav,"This is another test.",examples/sad.wav,0.8,"melancholic",100
            ```
            """)
        
        # Event handlers for batch tab
        def update_custom_dir_visibility(option):
            return gr.update(visible=(option == "custom"))
        
        output_dir_option.change(
            update_custom_dir_visibility,
            inputs=[output_dir_option],
            outputs=[custom_output_dir]
        )
        
        batch_gen_button.click(
            gen_batch,
            inputs=[batch_input, output_dir_option, custom_output_dir],
            outputs=[batch_result, batch_sample_output]
        )


if __name__ == "__main__":
    demo.queue(20)
    demo.launch(server_name=cmd_args.host, server_port=cmd_args.port, share=True)
